The DataPipelineManager is a Python module designed for end-to-end ETL automation. It contains a class ETLProcessor with methods extract(source: str) -> pd.DataFrame, transform(df: pd.DataFrame, rules: dict) -> pd.DataFrame, and load(df: pd.DataFrame, destination: str, mode: str = "append") -> bool. The extract method supports APIs, CSV files, and SQL databases as input sources, while transform applies user-defined mapping rules, filters, and aggregations. The load function integrates with PostgreSQL and Snowflake, supporting bulk and incremental inserts.

For scheduling, the ETLProcessor is orchestrated using Apache Airflow DAGs, where each task calls the appropriate method with parameters defined in dag_config.yaml. Logging is handled by the Logger class with functions info(msg: str) and error(msg: str, code: int) to standardize logs. Exception handling uses a decorator @retry_on_failure(max_retries: int = 3, delay: int = 5) that retries failed tasks.

In the ML workflow, the module includes a ModelTrainer class with functions train_model(data: pd.DataFrame, model_type: str = "XGBoost", params: dict = {}) -> Any and evaluate_model(model: Any, test_data: pd.DataFrame) -> dict. The trainer supports hyperparameter tuning, cross-validation, and saving models via pickle or joblib. The pipeline automatically triggers notifications through Notifier.send_email(recipient: str, subject: str, body: str) on completion or failure.

Dependencies include pandas, numpy, scikit-learn, xgboost, sqlalchemy, and smtplib, and all modules are containerized using Docker for deployment in AWS ECS.